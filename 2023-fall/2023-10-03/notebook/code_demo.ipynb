{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**You can also open this notebook in Google Colab**)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/xiangshiyin/data-programming-with-python/blob/main/2023-fall/2023-10-03/notebook/code_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question from last class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set data type of columsn with missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tb1 = pd.DataFrame({'key': ['foo', 'boo', 'foo'], 'lval': [1, 2, 3]})\n",
    "tb2 = pd.DataFrame({'key': ['foo', 'coo'], 'rval': [5, 6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join = pd.merge(tb1, tb2, on='key', how='outer')\n",
    "outer_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join.lval.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workarounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "- `df.astype()` function in detail: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html\n",
    "- Stackoverflow: https://stackoverflow.com/questions/48511484/data-type-conversion-error-valueerror-cannot-convert-non-finite-values-na-or\n",
    "- Nullable integer type: https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html#integer-na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join.lval.fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join.lval.astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_join.lval.astype(np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instead of asking \"what could happen\", try and break the code\n",
    "* Research online and read documentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dataframe` practice (continued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees = pd.read_excel('../data/Employees.xls')\n",
    "Territory = pd.read_excel('../data/SalesTerritory.xls')\n",
    "Customers = pd.read_excel('../data/Customers.xls')\n",
    "Orders = pd.read_excel('../data/ItemsOrdered.xls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter\n",
    "### Change the logic to include anyone who meets any of the 3 conditions (i.e., people who are either married, live in Washington state, or have more than 35 vacation hours left)\n",
    "\n",
    "SQL logic\n",
    "```sql\n",
    "SELECT \t\n",
    "  e.EmployeeID\n",
    "  ,e.FirstName\n",
    "  ,e.LastName\n",
    "  ,e.MaritalStatus\n",
    "  ,e.VacationHours\n",
    "  ,e.SalariedFlag\n",
    "  ,e.StateProvinceName\n",
    "  ,e.CountryName\n",
    "FROM dbo.Employees AS e\n",
    "WHERE \n",
    "  e.MaritalStatus = 'M' \n",
    "  OR e.VacationHours > 35 \n",
    "  OR e.StateProvinceName = 'Washington'\n",
    "\t;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.loc[(Employees.MaritalStatus=='M')|(Employees.VacationHours>35)|(Employees.StateProvinceName=='Washington'), \n",
    "              ['EmployeeID', 'FirstName', 'LastName','MaritalStatus','VacationHours','SalariedFlag','StateProvinceName','CountryName']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.loc[(Employees.MaritalStatus=='M')|(Employees.VacationHours>35)|(Employees.StateProvinceName=='Washington'), \n",
    "              ['EmployeeID', 'FirstName', 'LastName','MaritalStatus','VacationHours','SalariedFlag','StateProvinceName','CountryName']].EmployeeID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins\n",
    "![](../pics/joins.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If any are salespeople then show me the details about their sales territory\n",
    "```sql\n",
    "SELECT e.EmployeeID ,e.FirstName + ' ' + e.LastName AS EmployeeName ,st.* \n",
    "FROM dbo.Employees AS e \n",
    "INNER JOIN dbo.SalesTerritory AS st ON e.TerritoryID = st.TerritoryID\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Territory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Territory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    Employees.loc[:,['TerritoryID','EmployeeID','FirstName','LastName']],\n",
    "    Territory,\n",
    "    on = 'TerritoryID',\n",
    "    how = 'inner'\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.EmployeeID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    Employees.loc[:,['TerritoryID','EmployeeID','FirstName','LastName']],\n",
    "    Territory,\n",
    "    on = 'TerritoryID',\n",
    "    how = 'left'\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EmployeeName'] = [\n",
    "    first + ' ' + last\n",
    "    for first,last in zip(df.FirstName, df.LastName)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For all sales territories, also show what customers fall under them\n",
    "```sql\n",
    "SELECT * \n",
    "FROM dbo.SalesTerritory AS st \n",
    "LEFT OUTER JOIN dbo.Customers AS c ON c.SalesTerritoryID = st.TerritoryID ;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Customers.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Territory.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Territory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    Territory,\n",
    "    Customers,\n",
    "#     on = 'TerritoryID',\n",
    "    left_on='TerritoryID',\n",
    "    right_on='SalesTerritoryID',\n",
    "    how = 'left'\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(\n",
    "    Territory,\n",
    "    Customers.rename(columns={'SalesTerritoryID':'TerritoryID'}),\n",
    "    on = 'TerritoryID',\n",
    "    how = 'left'\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any sales territories that don't have any customers associated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[condition, column_list]\n",
    "df[df.CustomerID.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.CustomerID.isna()].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "Reading Materials: \n",
    "* (official doc): https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html\n",
    "* (summary) https://www.shanelynn.ie/summarising-aggregation-and-grouping-data-in-python-pandas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the earliest birthdate for all employees?\n",
    "\n",
    "SQL logic\n",
    "```sql\n",
    "SELECT MIN(e.BirthDate) FROM dbo.Employees AS e;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Employees.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.loc[:,['BirthDate']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.dtypes['BirthDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(Employees.dtypes['BirthDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.BirthDate.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "- Lexicographic order: https://en.wikipedia.org/wiki/Lexicographic_order\n",
    "- StackOverflow: https://stackoverflow.com/questions/45950646/what-is-lexicographical-order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'1970-01-01' < '2023-06-26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.BirthDate.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.BirthDate.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.BirthDate.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to the above, the most recent birthdate for all employees\n",
    "\n",
    "SQL logic\n",
    "```sql\n",
    "SELECT \n",
    "  MIN(e.BirthDate) AS 'Earliest Birthday'\n",
    "  , MAX(e.BirthDate) AS 'Most Reecent Birthday'\n",
    "FROM dbo.Employees AS e;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [4,5,1,2,3]\n",
    "min(x), max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.agg({'BirthDate':['min','max']}).T\n",
    "\n",
    "# Employees.agg({'BirthDate':['min','max']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.agg({'BirthDate':[min,max]}).T.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the above results broken down by gender\n",
    "\n",
    "SQL logic\n",
    "```sql\n",
    "SELECT \n",
    "  e.Gender\n",
    "  , MIN(e.BirthDate) AS 'Earliest Birthday'\n",
    "  , MAX(e.BirthDate) AS 'Most Reecent Birthday'\n",
    "FROM dbo.Employees AS e\n",
    "GROUP BY e.Gender\n",
    ";\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.groupby('Gender')['BirthDate'].min().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.groupby('Gender').agg({'BirthDate':[min,max]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.groupby('Gender').agg(\n",
    "    min_bday=('BirthDate',min),\n",
    "    max_bday=('BirthDate',max)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the above results broken down by gender, and salaried/hourly\n",
    "\n",
    "SQL logic\n",
    "```sql\n",
    "SELECT \n",
    "  e.Gender\n",
    "  , e.SalariedFlag\n",
    "  , MIN(e.BirthDate) AS 'Earliest Birthday'\n",
    "  , MAX(e.BirthDate) AS 'Most Reecent Birthday'\n",
    "FROM dbo.Employees AS e\n",
    "GROUP BY e.Gender, e.SalariedFlag\n",
    ";\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.groupby(['Gender','SalariedFlag']).agg(\n",
    "    min_bday=('BirthDate',min),\n",
    "    max_bday=('BirthDate',max)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the average vacation hours for all employees?\n",
    "\n",
    "SQL logic\n",
    "```sql\n",
    "SELECT AVG(e.VacationHours)\n",
    "FROM dbo.Employees AS e\t\n",
    ";\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.VacationHours.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the above results broken down and ordered by job title¶\n",
    "\n",
    "SQL logic\n",
    "```sql\n",
    "SELECT \n",
    "  e.JobTitle\n",
    "  , AVG(e.VacationHours) AS 'Average Vacation'\n",
    "  , MIN(e.VacationHours) AS 'Minimum Vacation'\n",
    "FROM dbo.Employees AS e\n",
    "GROUP BY e.JobTitle\n",
    ";\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.groupby('JobTitle')['VacationHours'].min().reset_index().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.groupby('JobTitle')['VacationHours'].mean().reset_index().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.groupby('JobTitle')['VacationHours'].apply(lambda x: sum(x)/len(x)).reset_index().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Employees.groupby('JobTitle').agg(\n",
    "    avg_pto_left=('VacationHours',lambda x: sum(x)/len(x)),\n",
    "    min_pto_left=('VacationHours',min)\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Employees.groupby('JobTitle').agg(\n",
    "    avg_pto_left=('VacationHours',lambda x: sum(x)/len(x)),\n",
    "    min_pto_left=('VacationHours',min)\n",
    ").reset_index()\n",
    "output.sort_values(by=['avg_pto_left'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Python Statistics Landscape\n",
    "\n",
    "There are many Python statistics libraries for you to work with.\n",
    "\n",
    "* **Foundation Libraries**\n",
    "    * `statistics`: built-in Python library for descriptive statistics (link: https://docs.python.org/3/library/statistics.html)\n",
    "    * `numpy`: numerical computing, numpy arrays\n",
    "    * `scipy`: scientific computing based on numpy, the `scipy.stats` module (link: https://docs.scipy.org/doc/scipy/reference/stats.html) covers a large number of probability distributions and statistical functions (link: https://www.scipy.org/)\n",
    "    \n",
    "* **Data Science Libraries**\n",
    "    * `pandas`: 1D and 2D labeled data manipulations and computation\n",
    "    * `statsmodels`: a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration (link: https://www.statsmodels.org/stable/index.html)\n",
    "    * `matplotlib`: graphs and visualization (link: https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistical Analysis\n",
    "\n",
    "Descriptive statistics is about describing and summarizing data. It uses two main approaches:\n",
    "\n",
    "* The quantitative approach describes and summarizes data numerically.\n",
    "* The visual approach illustrates data with charts, plots, histograms, and other graphs.\n",
    "\n",
    "You can apply descriptive statistics to one or many datasets or variables. When you describe and summarize a single variable, you’re performing univariate analysis. When you search for statistical relationships among a pair of variables, you’re doing a bivariate analysis. Similarly, a multivariate analysis is concerned with multiple variables at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Case Study]**\n",
    "\n",
    "**Atlanta Police Department Crime Data** ![APD Logo](https://atlantapd.galls.com/photos/partners/atlantapd/logo.jpg)\n",
    "\n",
    "\n",
    "The Atlanta Police Department provides raw crime data at http://www.atlantapd.org/i-want-to/crime-data-downloads\n",
    "- Atlanta police open data portal: https://opendata.atlantapd.org/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the 2009-2019 crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/COBRA-2009-2019.csv',sep=',',header=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOY crime report volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['Report Date'].map(lambda x: x[:4])\n",
    "x.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rpt_yr'] = df['Report Date'].map(lambda x: x[:4])\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, ['Report Number', 'Report Date', 'rpt_yr']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of reports every year\n",
    "# df['rpt_yr'] = df['Report Date'].map(lambda x: x[:4])\n",
    "\n",
    "num_rpt_by_yr = df.groupby('rpt_yr').agg(\n",
    "    num_row=('Report Number',len),\n",
    "    num_rpt=('Report Number',lambda x: len(set(x)))\n",
    ").reset_index()\n",
    "num_rpt_by_yr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('rpt_yr')['Report Number'].apply(lambda x: len(x)).reset_index().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowcount = df.shape[0]\n",
    "num_unique_report_number = df['Report Number'].nunique()\n",
    "print(rowcount)\n",
    "print(num_unique_report_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('rpt_yr')['Report Number'].apply(lambda x: len(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Shift Occurence'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on crime report volume by shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of cases per shift in 2019\n",
    "num_rpt_by_shift = df[df.rpt_yr=='2019'].groupby('Shift Occurence').agg(\n",
    "    num_rpt=('Report Number',lambda x: len(set(x)))\n",
    ").reset_index()\n",
    "num_rpt_by_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## number of cases per shift in the past 3 years\n",
    "num_rpt_by_yr_shift = df[df.rpt_yr>='2017'].groupby(['rpt_yr','Shift Occurence']).agg(\n",
    "    num_rpt=('Report Number',lambda x: len(set(x)))\n",
    ").reset_index()\n",
    "num_rpt_by_yr_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rpt_by_yr_shift.sort_values(by=['Shift Occurence','rpt_yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rpt_by_yr_shift.sort_values(by=['rpt_yr', 'num_rpt'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## % of cases per shift in the past 3 years\n",
    "num_rpt_by_yr_shift2 = pd.merge(\n",
    "    num_rpt_by_yr_shift,\n",
    "    num_rpt_by_yr.loc[:,['rpt_yr','num_rpt']].copy().rename(columns={'num_rpt':'annual_total'}),\n",
    "    on='rpt_yr'\n",
    ")\n",
    "num_rpt_by_yr_shift2\n",
    "\n",
    "# num_rpt_by_yr_shift2.sort_values(by=['Shift Occurence','rpt_yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rpt_by_yr_shift2['percent'] = [\n",
    "    round(subtotal/total,2)\n",
    "    for subtotal,total in zip(num_rpt_by_yr_shift2.num_rpt,num_rpt_by_yr_shift2.annual_total)\n",
    "]\n",
    "num_rpt_by_yr_shift2.sort_values(by=['Shift Occurence','rpt_yr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the YOY change of % by shift with bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rpt_by_yr_shift2.rpt_yr = num_rpt_by_yr_shift2.rpt_yr.astype(int)\n",
    "dw = num_rpt_by_yr_shift2[num_rpt_by_yr_shift2['Shift Occurence']=='Day Watch']\n",
    "ew = num_rpt_by_yr_shift2[num_rpt_by_yr_shift2['Shift Occurence']=='Evening Watch']\n",
    "mw = num_rpt_by_yr_shift2[num_rpt_by_yr_shift2['Shift Occurence']=='Morning Watch']\n",
    "unk = num_rpt_by_yr_shift2[num_rpt_by_yr_shift2['Shift Occurence']=='Unknown']\n",
    "\n",
    "plt.plot(dw.rpt_yr, dw.percent, '-o', label='day watch')\n",
    "plt.plot(ew.rpt_yr, ew.percent, '-o', label='evening watch')\n",
    "plt.plot(mw.rpt_yr, mw.percent, '-o', label='morning watch')\n",
    "plt.plot(unk.rpt_yr, unk.percent, '-o', label='unknown')\n",
    "\n",
    "plt.xticks(ticks=[2017,2018,2019])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Test and Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal distribution\n",
    "\n",
    "The Normal distribution or Gaussian distribution is by far the most important of all the distribution functions. This is due to the fact that the mean values of all distribution functions approximate a normal distribution for large enough sample numbers. Mathematically, the normal distribution is characterized by a mean value $\\mu$, and a standard deviation $\\sigma$:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "$$\n",
    "\n",
    "When $\\mu=0$ and $\\sigma=1$, the distribution is called the `standard normal distribution`:\n",
    "$$\n",
    "f(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\n",
    "$$\n",
    "\n",
    "The **68-95-997** rule:\n",
    "![](https://miro.medium.com/max/24000/1*IZ2II2HYKeoMrdLU5jW6Dw.png)\n",
    "\n",
    "| Range      | Probability within range | Probability outside range |\n",
    "|------------|--------------------------|---------------------------|\n",
    "| Mean ± 1SD | 68.3%                    | 31.7%                     |\n",
    "| Mean ± 2SD | 95.4%                    | 4.6%                      |\n",
    "| Mean ± 3SD | 99.7%                    | 0.27%                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate random samples from the normal distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rvs = stats.norm.rvs(loc=0, scale=1, size=5, random_state=123)\n",
    "rvs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `seed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = stats.norm.rvs(loc=0, scale=1, size=5, random_state=123) # seed\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = stats.norm.rvs(loc=0, scale=1, size=5)\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = stats.norm.rvs(loc=0, scale=1, size=5)\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = stats.norm.rvs(loc=0, scale=1, size=5, random_state=12345)\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "np.random.randn(5) # Return a sample (or samples) from the \"standard normal\" distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For random samples from $N(\\mu, \\sigma^2)$, use:\n",
    "\n",
    "$$\\sigma \\cdot np.random.randn(...) + \\mu$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Any way we could \"validate\" if this is indeed sampled from normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(np.random.randn(10000))\n",
    "plt.hist(np.random.randn(10000),density=True)\n",
    "plt.title('Histogram of random samples from standard normal distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.pdf(x=[-3,-2,-1,1,2,3],loc=0,scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you could also do\n",
    "distNorm = stats.norm(loc=0,scale=1)\n",
    "distNorm.pdf(x=[-3,-2,-1,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the histogram from random sample\n",
    "plt.hist(np.random.randn(10000),density=True,label='random sample') \n",
    "# construct the pdf curve\n",
    "xs = np.linspace(start=-4,stop=4,num=100)\n",
    "ys = stats.norm.pdf(x=xs)\n",
    "plt.plot(xs,ys,label='norm pdf')\n",
    "\n",
    "plt.title('hist vs. pdf (Normal Distribution)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate CDF** (Cumulative Distribution Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.cdf(x=[-3,-2,-1,1,2,3],loc=0,scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the histogram from random sample\n",
    "plt.hist(np.random.randn(10000),density=True,label='random sample') \n",
    "# construct the pdf curve\n",
    "xs = np.linspace(start=-4,stop=4,num=100)\n",
    "ys_pdf = stats.norm.pdf(x=xs)\n",
    "plt.plot(xs,ys_pdf,label='norm pdf')\n",
    "\n",
    "# construct the cdf curve\n",
    "ys_cdf = stats.norm.cdf(x=xs)\n",
    "plt.plot(xs,ys_cdf,label='norm cdf')\n",
    "\n",
    "plt.title('hist vs. pdf (Normal Distribution)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ppf`: Percent Point Function (Inverse of CDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.ppf([0.05,0.95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Central limit theorem**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Central_limit_theorem\n",
    "\n",
    "In probability theory, the **central limit theorem (CLT)** establishes that in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a bell curve), even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory, because it implies that probabilistic and statistical methods that work for normal distributions can be applied to many problems involving other types of distributions.\n",
    "\n",
    "More specifically, central limit theorem states that if $ X_{1},X_{2},...,X_{n}$ are each a random sample of size $n$, taken from a population with mean $\\mu$ and finite variance $\\sigma^2$ and if $\\bar{X}$ is the sample mean, then the limiting form of the distribution of $Z=\\frac {{\\bar {X}}_{n}-\\mu }{\\sigma /\\surd n}$ as $n\\to \\infty$, is the standard normal distribution.\n",
    "\n",
    "![](https://i.ytimg.com/vi/4YLtvNeRIrg/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $t$ distribution\n",
    "\n",
    "The sample distribution of mean values for samples from a normally distributed population. Typically used for small sample numbers, when the true mean/SD are not known.\n",
    "\n",
    "If $\\bar{x}$ is the sample mean, and $s$ is the sample standard deviation, then\n",
    "$$\n",
    "\\frac{\\bar{x}-\\mu}{s/\\sqrt{n}} \\sim t_{\\nu}\n",
    "$$ where $\\nu=n-1$ represents the degree of freedom, and $n$ is the sample size.\n",
    "\n",
    "When $n$ is large enough, $t$ distribution asymptotically approaches standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random numbers from the t distribution\n",
    "\n",
    "n = 20\n",
    "df = n - 1\n",
    "rvs = stats.t.rvs(df=df,size=5,random_state=123)\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs = stats.t.rvs(df,size=5,random_state=123)\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pdf of t distribution\n",
    "xs = np.linspace(start=-4,stop=4,num=100)\n",
    "ys_1 = stats.t.pdf(x=xs,df=1)\n",
    "ys_5 = stats.t.pdf(x=xs,df=5)\n",
    "ys_10 = stats.t.pdf(x=xs,df=10)\n",
    "ys_20 = stats.t.pdf(x=xs,df=20)\n",
    "ys_100 = stats.t.pdf(x=xs,df=100)\n",
    "plt.plot(xs,ys_1,label='df=1')\n",
    "plt.plot(xs,ys_5,label='df=5')\n",
    "plt.plot(xs,ys_10,label='df=10')\n",
    "plt.plot(xs,ys_20,label='df=20')\n",
    "plt.plot(xs,ys_100,label='df=100')\n",
    "\n",
    "# plot the pdf of standard normal distribution\n",
    "ys = stats.norm.pdf(x=xs)\n",
    "plt.plot(xs,ys,'o',label='norm pdf')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cdf of t distribution\n",
    "xs = np.linspace(start=-4,stop=4,num=100)\n",
    "ys_1 = stats.t.cdf(x=xs,df=1)\n",
    "ys_5 = stats.t.cdf(x=xs,df=5)\n",
    "ys_10 = stats.t.cdf(x=xs,df=10)\n",
    "ys_20 = stats.t.cdf(x=xs,df=20)\n",
    "ys_100 = stats.t.cdf(x=xs,df=100)\n",
    "plt.plot(xs,ys_1,label='df=1')\n",
    "plt.plot(xs,ys_5,label='df=5')\n",
    "plt.plot(xs,ys_10,label='df=10')\n",
    "plt.plot(xs,ys_20,label='df=20')\n",
    "plt.plot(xs,ys_100,label='df=100')\n",
    "# plot the cdf of standard normal distribution\n",
    "ys = stats.norm.cdf(x=xs)\n",
    "plt.plot(xs,ys,'o',label='norm cdf')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ppf`: Percent Point Function (Inverse of CDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when n=20, df = n-1 =19\n",
    "stats.t.ppf([0.05,0.95],df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\chi^2 distribution$\n",
    "$\\chi^2$ (chi-square) distribution describes the distribution of the summed squares of random variates from a standard normal distribution. The sum squares of $n$ independent random samples from standard normal distribution follows a chi-square distribution of $n$ degrees of freedom:\n",
    "$$\n",
    "\\sum_{i=1}^{n}X_i^2 \\sim \\chi_n^2\n",
    "$$\n",
    "For $n$ independent random samples from normal distribution with a standard deviation of $\\sigma$, the following test statistic follows the chi-square distribution of $n-1$ degrees of freedom:\n",
    "$$\n",
    "\\sum_{i=1}^{n}(\\frac{X_i-\\bar{X}}{\\sigma})^2 = (n-1)\\frac{s}{\\sigma^2} \\sim \\chi_{n-1}^2\n",
    "$$\n",
    "where $s$ stands for sample standard deviation. This can be used in hypothesis test of comparison between sample standard deviation and population standard deviation.\n",
    "\n",
    "It is also commonly used in statistical independence or association between two or more categorical variables using the following test statistic (with the help of a contingency table):\n",
    "$$\n",
    "\\sum_{j=1}^{n}\\sum_{i=1}^{m}\\frac{({frequency}_{observed} - {frequency}_{expected})^2}{{frequency}_{expected}} \\sim \\chi_{df}^2\n",
    "$$\n",
    "where \n",
    "$$\n",
    "df = (m-1) \\cdot (n-1)\n",
    "$$ \n",
    "see example here: http://sites.utexas.edu/sos/guided/inferential/categorical/chi2/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "df = n-1\n",
    "rvs = stats.chi2.rvs(df=df,size=5,random_state=123)\n",
    "rvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pdf of chi2 distribution\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "xs = np.linspace(start=0,stop=100,num=1000)\n",
    "ys_1 = stats.chi2.pdf(x=xs,df=1)\n",
    "ys_2 = stats.chi2.pdf(x=xs,df=2)\n",
    "ys_3 = stats.chi2.pdf(x=xs,df=3)\n",
    "ys_4 = stats.chi2.pdf(x=xs,df=4)\n",
    "ys_6 = stats.chi2.pdf(x=xs,df=6)\n",
    "ys_9 = stats.chi2.pdf(x=xs,df=9)\n",
    "plt.plot(xs,ys_1,label='df=1')\n",
    "plt.plot(xs,ys_2,label='df=2')\n",
    "plt.plot(xs,ys_3,label='df=3')\n",
    "plt.plot(xs,ys_4,label='df=4')\n",
    "plt.plot(xs,ys_6,label='df=6')\n",
    "plt.plot(xs,ys_9,label='df=9')\n",
    "plt.xlim(0,20)\n",
    "plt.ylim(0,0.5)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cdf of chi2 distribution\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "xs = np.linspace(start=0,stop=100,num=1000)\n",
    "ys_1 = stats.chi2.cdf(x=xs,df=1)\n",
    "ys_2 = stats.chi2.cdf(x=xs,df=2)\n",
    "ys_3 = stats.chi2.cdf(x=xs,df=3)\n",
    "ys_4 = stats.chi2.cdf(x=xs,df=4)\n",
    "ys_6 = stats.chi2.cdf(x=xs,df=6)\n",
    "ys_9 = stats.chi2.cdf(x=xs,df=9)\n",
    "plt.plot(xs,ys_1,label='df=1')\n",
    "plt.plot(xs,ys_2,label='df=2')\n",
    "plt.plot(xs,ys_3,label='df=3')\n",
    "plt.plot(xs,ys_4,label='df=4')\n",
    "plt.plot(xs,ys_6,label='df=6')\n",
    "plt.plot(xs,ys_9,label='df=9')\n",
    "plt.xlim(0,20)\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ppf`: Percent Point Function (Inverse of CDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when df=9\n",
    "stats.chi2.ppf([0.05,0.95],df=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Examine statistically whether boys got in trouble in school more often\n",
    "- The distribution\n",
    "\n",
    "| Segment| Got in Trouble| Did Not Get in Trouble|Total|\n",
    "|----------|----------|----------|----------|\n",
    "| Boys| 46|71|117|\n",
    "| Girls| 37|83|120|\n",
    "|Total|83|154|237|\n",
    "\n",
    "- The hypotehsis\n",
    "  - $H_0$: There is no relationship between gender and getting in trouble at school\n",
    "  - $H_1$: There is a relationship between gender and getting in trouble at school\n",
    "- The test statistic $\\chi^2 = \\sum{\\frac{(O-E)^2}{E}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = np.array([46, 71, 37, 83])\n",
    "E = np.array([83 * 117 / 237, 154 * 117 / 237, 83 * 120 /237, 154 * 120/237])\n",
    "chi2 = np.sum((O-E) ** 2 / E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.chi2.ppf([0.05,0.95],df=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $F$ distribution\n",
    "\n",
    "This distribution is named after Sir Ronald Fisher, who developed the F distribution for use in determining critical values in **ANOVAs** (`Analysis Of Variance`).\n",
    "\n",
    "If we want to investigate whether two groups have the same variance, we have to calculate the ratio of the sample standard deviations squared (assume $S_1^2 > S_2^2$):\n",
    "$$\n",
    "\\frac{S_1^2}{S_2^2} \\sim F_{df_1,df_2} = \\frac{\\chi_{df_1}^2/df_1}{\\chi_{df_2}^2/df_2} \\sim F_{N_1-1,N_2-1}\n",
    "$$\n",
    "where $\\chi_{df_1}^2$ and $\\chi_{df_2}^2$ are the chi-squared statistics of sample one and two respectively, and $df_1$ and $df_2$ are their degrees of freedom, in which case\n",
    "$$\n",
    "df_1 = N_1-1\n",
    "$$\n",
    "and\n",
    "$$\n",
    "df_2 = N_2-1\n",
    "$$ ($N_1$ and $N_2$ are sample sizes of the two samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the pdf of F distribution\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "xs = np.linspace(start=0,stop=5,num=1000)\n",
    "ys_11 = stats.f.pdf(x=xs,dfn=1,dfd=1)\n",
    "ys_21 = stats.f.pdf(x=xs,dfn=2,dfd=1)\n",
    "ys_52 = stats.f.pdf(x=xs,dfn=5,dfd=2)\n",
    "ys_100100 = stats.f.pdf(x=xs,dfn=100,dfd=100)\n",
    "plt.plot(xs,ys_11,label='F(1/1)')\n",
    "plt.plot(xs,ys_21,label='F(2/1)')\n",
    "plt.plot(xs,ys_52,label='F(5/2)')\n",
    "plt.plot(xs,ys_100100,label='F(100/100)')\n",
    "\n",
    "plt.xlim(0,3)\n",
    "plt.ylim(0,3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cdf of F distribution\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "xs = np.linspace(start=0,stop=5,num=1000)\n",
    "ys_11 = stats.f.cdf(x=xs,dfn=1,dfd=1)\n",
    "ys_21 = stats.f.cdf(x=xs,dfn=2,dfd=1)\n",
    "ys_52 = stats.f.cdf(x=xs,dfn=5,dfd=2)\n",
    "ys_100100 = stats.f.cdf(x=xs,dfn=100,dfd=100)\n",
    "plt.plot(xs,ys_11,label='F(1/1)')\n",
    "plt.plot(xs,ys_21,label='F(2/1)')\n",
    "plt.plot(xs,ys_52,label='F(5/2)')\n",
    "plt.plot(xs,ys_100100,label='F(100/100)')\n",
    "\n",
    "plt.xlim(0,3)\n",
    "plt.ylim(0,1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`ppf`: Percent Point Function (Inverse of CDF)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when dfn=100, dfd=100\n",
    "stats.f.ppf([0.05,0.95],dfn=100,dfd=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Test\n",
    "\n",
    "wikipedia: https://en.wikipedia.org/wiki/Statistical_hypothesis_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Hypothesis` is a statement about a parameter. A `hypothesis test` is a standard procedure to test a statement (the `hypothesis`), and typically we need to select between two complementary `hypothesis`:\n",
    "* `Null hypothesis` ($H_0$): A statment about an established fact of a parameter. The null hypothesis is generally assumed to be true until evidence indicates otherwise (similar to the case that a defendant of a jury trial is presumed innocent until proven guilty). It is normally expressed as Math equation, and **it must contain a condition of equality, such as $=,\\geq, \\leq $**.\n",
    "* `Alternative hypothesis` ($H_1$): A statement that the parameter has a value that differs from the null hypothesis.\n",
    " Needs a strong support from data to change our thinking and contradicts Ho. Expressed as Math statement it contains $\\neq, <, >$.\n",
    "\n",
    "We also need a `test statistic` (a quantity derived from the sample). Typically it is selected or defined in such a way as to quantify, within observed data, behaviours that would distinguish the `null` from the `alternative hypothesis`, where such an alternative is prescribed, or that would characterize the null hypothesis if there is no explicitly stated alternative hypothesis. Normally, we should have a good idea on the sampling distribution of the test statistic.\n",
    "* List of commonly used `test statistic`: https://en.wikipedia.org/wiki/Test_statistic\n",
    "\n",
    "| Null Hypothesis        | Alternative Hypothesis      | Type of Alternative |\n",
    "|------------------------|-----------------------------|---------------------|\n",
    "|                        | $H_1$: $\\theta < \\theta_0$    | lower one-sided     |\n",
    "| $H_0$: $\\theta=\\theta_0$ | $H_1$: $\\theta > \\theta_0$    | upper one-sided     |\n",
    "|                        | $H_1$: $\\theta \\neq \\theta_0$ | two-sided           |\n",
    "\n",
    "|                                   | $H_0$ is true (Truly not guilty) |    $H_1$ is true (Truly guilty)   |\n",
    "|-----------------------------------|---------------------------|----------------------------|\n",
    "|  Accept null hypothesis Acquittal |        Right decision       | Wrong decision **Type II Error** |\n",
    "| Reject null hypothesis Conviction | Wrong decision **Type I Error** |        Right decision        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Population Proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: How to tell if a coin is fair?**\n",
    "\n",
    "**Problem**: Suppose we tossed a coin 100 times and we have obtained 38\n",
    "Heads and 62 Tails. Is the coin biased toward tails? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0$: $p_{head} = 0.5$\n",
    "\n",
    "$H_1$: $p_{head} \\neq 0.5$ (two-sided)\n",
    "\n",
    "Significance level $\\alpha=0.05$\n",
    "\n",
    "`Test statistic`: $z = \\frac{\\hat{p}-p_0}{SD(p_0)} \\sim N(0,1)$ according to `central limit theorem`, where $SD(p_0) =\\sqrt{\\frac{p_0q_0}{n}} = \\sqrt{\\frac{p_0(1-p_0)}{n}} $.\n",
    "![](https://www.investopedia.com/thmb/pF9cbALKXUA617NzyoKozi1B0rQ=/954x380/filters:no_upscale():max_bytes(150000):strip_icc()/Clipboard01-5c94e6b446e0fb00010ae8ed.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "p = 38/n\n",
    "sd = (p*(1-p)/n)**0.5\n",
    "z = (p-0.5)/sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2 * stats.norm.cdf(z) # two-sided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.norm.ppf([0.025,0.975]) # the 5% confidence interval boundary for two-sided alternative hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do t-test: $t = \\frac{\\hat{p}-p_0}{SD(p_0)} \\sim t_{n-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2 * stats.t.cdf(z, df = n-1) # two-sided\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.t.ppf([0.025,0.975], df=n-1) # the 5% confidence interval boundary for two-sided alternative hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `statsmodels` library to do the z-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.proportions_ztest(count=38,nobs=100,value=0.5,alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Population Proportion Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: A car manufacturer aims to improve the quality of the products by reducing the defects and also increase the customer satisfaction. Therefore, he monitors the efficiency of two assembly lines in the shop floor. In line A there are 18 defects reported out of 200 samples. While the line B shows 25 defects out of 600 cars. At α 5%, is the differences between two assembly procedures are significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0$: $p_1 - p_2 = 0$\n",
    "\n",
    "$H_1$: $p_1 - p_2 \\neq 0$ (two-sided)\n",
    "\n",
    "Significance level $\\alpha=0.05$\n",
    "\n",
    "`Test statistic`: $z = \\frac{\\hat{p_1}-\\hat{p_2} - 0}{SD} \\sim N(0,1)$ according to `central limit theorem`, where $SD = \\sqrt{p_0(1-p_0)(\\frac{1}{n_1}+\\frac{1}{n_2})}$, and $p_0 = \\frac{x_1+x_2}{n_1+n_2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "x1 = 18\n",
    "n1 = 200\n",
    "x2 = 25\n",
    "n2 = 600\n",
    "\n",
    "p1 = x1/n1\n",
    "p2 = x2/n2\n",
    "p0 = (x1+x2)/(n1+n2)\n",
    "# sd = math.sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)\n",
    "sd = math.sqrt(p0*(1-p0)*(1/n1 + 1/n2))\n",
    "z = (p1-p2)/sd\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2 * (1-stats.norm.cdf(z)) # two-sided\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `statsmodels` library to do the z-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.proportions_ztest(\n",
    "    count=np.array([x1,x2]),\n",
    "    nobs=np.array([n1,n2]),\n",
    "    value=0,\n",
    "    alternative='two-sided'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.proportions_ztest(\n",
    "    count=np.array([x1,x2]),\n",
    "    nobs=np.array([n1,n2]),\n",
    "    value=0,\n",
    "    alternative='larger'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Population Mean\n",
    "\n",
    "**Problem**: Your company wants to improve sales. Past sales data indicate that the average sale was \\\\$100 per transaction. After training your sales force, recent sales data (taken from a sample of 25 salesmen) indicates an average sale of \\\\$130, with a standard deviation of \\\\$15. Did the training work? Test your hypothesis at a 5\\% alpha level.\n",
    "\n",
    "$H_0$: $\\mu = \\mu_0$\n",
    "\n",
    "$H_1$: $\\mu \\geq \\mu_0$ (upper one-side)\n",
    "\n",
    "Significance level $\\alpha=0.05$\n",
    "\n",
    "`z-test`: $z = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\sim N(0,1)$, where $s$ is the sample standard deviation\n",
    "\n",
    "`t-test`: $t = \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1}$, where $s$ is the sample standard deviation, and $n$ is the sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## z-test\n",
    "mu0 = 100\n",
    "xbar = 130\n",
    "n = 25\n",
    "s = 15\n",
    "\n",
    "z = (xbar - mu0)/(s/math.sqrt(n))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1-stats.norm.cdf(z)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## t-test\n",
    "t = z\n",
    "p = 1-stats.t.cdf(t, df=n-1)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Population Mean Difference\n",
    "\n",
    "**Problem #1**: Does right‐ or left‐handedness affect how fast people type? Random samples of students from a typing class are given a typing speed test (words per minute), and the results are compared. Significance level for the test: 0.10. Because you are looking for a difference between the groups in either direction (right‐handed faster than left, or vice versa), this is a two‐tailed test.\n",
    "\n",
    "| Group | Handedness | n  | $\\bar{x}$ | s   |\n",
    "|-------|------------|----|-----------|-----|\n",
    "| 1     | Left       | 9  | 59.3      | 4.3 |\n",
    "| 2     | Right      | 16 | 55.8      | 5.7 |\n",
    "\n",
    "$H_0$: $\\mu_1 - \\mu_2 = 0$\n",
    "\n",
    "$H_1$: $\\mu_1 - \\mu_2 \\neq 0$ (two-sided)\n",
    "\n",
    "Significance level $\\alpha=0.05$\n",
    "\n",
    "Like before, assume the two groups have the same variance, we could do either `z-test` or `t-test`.\n",
    "$$\n",
    "\\frac{\\bar{x_1} - \\bar{x_2} - 0}{\\sqrt{s_p^2(\\frac{1}{n_1} + \\frac{1}{n_2})}} \\sim N(0,1)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\frac{\\bar{x_1} - \\bar{x_2} - 0}{\\sqrt{s_p^2(\\frac{1}{n_1} + \\frac{1}{n_2})}} \\sim t_{n_1+n_2-2}\n",
    "$$\n",
    "Here, $s_p$ is the pooled variance $s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 9\n",
    "n2 = 16\n",
    "\n",
    "xbar1 = 59.3\n",
    "xbar2 = 55.8\n",
    "\n",
    "s1 = 4.3\n",
    "s2 = 5.7\n",
    "\n",
    "sp = math.sqrt(((n1-1)*(s1**2) + (n2-1)*(s2**2))/(n1+n2-2))\n",
    "sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z test\n",
    "z = (xbar1 - xbar2)/(sp*math.sqrt(1/n1+1/n2))\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (1-stats.norm.cdf(z))*2\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## t-test\n",
    "t = z\n",
    "p = (1-stats.t.cdf(t, df=n1+n2-2))*2\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem #2**: An experiment is conducted to determine whether intensive tutoring (covering a great deal of material in a fixed amount of time) is more effective than paced tutoring (covering less material in the same amount of time). Two randomly chosen groups are tutored separately and then administered proficiency tests. Use a significance level of α < 0.05.\n",
    "\n",
    "| Group | Method | n  | $\\bar{x}$ | s   |\n",
    "|-------|------------|----|-----------|-----|\n",
    "| 1     | Intensive       | 12  | 46.31      | 6.44 |\n",
    "| 2     | Paced      | 10 | 42.79      | 7.52 |\n",
    "\n",
    "\n",
    "$H_0$: $\\mu_1 - \\mu_2 = 0$\n",
    "\n",
    "$H_1$: $\\mu_1 - \\mu_2 \\neq 0$ (two-sided)\n",
    "\n",
    "Significance level $\\alpha=0.05$\n",
    "\n",
    "Like before, assume the two groups have the same variance, we could do either `z-test` or `t-test`.\n",
    "$$\n",
    "\\frac{\\bar{x_1} - \\bar{x_2} - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\sim N(0,1)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\frac{\\bar{x_1} - \\bar{x_2} - 0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}} \\sim t_{n_1+n_2-2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = 12\n",
    "n2 = 10\n",
    "\n",
    "xbar1 = 46.31\n",
    "xbar2 = 42.79\n",
    "\n",
    "s1 = 6.44\n",
    "s2 = 7.52\n",
    "\n",
    "# z-test\n",
    "z = (xbar1-xbar2)/math.sqrt(s1**2/n1 + s2**2/n2)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (1-stats.norm.cdf(z))*2\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-test\n",
    "\n",
    "t = z\n",
    "p = (1-stats.t.cdf(t, df=n1+n2-2))*2\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-programming-with-python-h2fMFruk-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
